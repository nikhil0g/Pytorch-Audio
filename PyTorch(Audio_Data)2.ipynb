{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e02615b",
   "metadata": {},
   "source": [
    "## Build the speech model\n",
    "### Now that we've created spectrogram images, it's time to build the computer vision model. If you're following along with the different modules in this PyTorch learning path, then you should have a good understanding of how to create a computer vision model (in particular, see the \"Introduction to Computer Vision with PyTorch\" Learn module). You'll be using the torchvision package to build your vision model. The convolutional neural network (CNN) layer (conv2d) will be used to extract the unique features from the spectrogram image for each speech command.\n",
    "\n",
    "### Let's import the packages we need to build the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bff7ff43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Downloading pandas-1.4.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 11.6 MB 1.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.21.0 in /home/nikhil/anaconda3/envs/pytorch/lib/python3.10/site-packages (from pandas) (1.23.0)\n",
      "Collecting pytz>=2020.1\n",
      "  Using cached pytz-2022.1-py2.py3-none-any.whl (503 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /home/nikhil/anaconda3/envs/pytorch/lib/python3.10/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/nikhil/anaconda3/envs/pytorch/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
      "Installing collected packages: pytz, pandas\n",
      "Successfully installed pandas-1.4.3 pytz-2022.1\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8fca205c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import torch\n",
    "import torchaudio\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import datasets, models, transforms\n",
    "from torchinfo import summary\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d328c3d",
   "metadata": {},
   "source": [
    "## Load spectrogram images into a data loader for training\n",
    "### Here, we provide the path to our image data and use PyTorch's ImageFolder dataset helper class to load the images into tensors. We'll also normalize the images by resizing to a dimension of 201 x 81."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6cdf280d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset ImageFolder\n",
      "    Number of datapoints: 7985\n",
      "    Root location: ./data/spectrograms\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               Resize(size=(201, 81), interpolation=bilinear, max_size=None, antialias=None)\n",
      "               ToTensor()\n",
      "           )\n"
     ]
    }
   ],
   "source": [
    "data_path = './data/spectrograms' #looking in subfolder train\n",
    "\n",
    "yes_no_dataset = datasets.ImageFolder(\n",
    "    root=data_path,\n",
    "    transform=transforms.Compose([transforms.Resize((201,81)),\n",
    "                                  transforms.ToTensor()\n",
    "                                  ])\n",
    ")\n",
    "print(yes_no_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a584834b",
   "metadata": {},
   "source": [
    "### ImageFolder automatically creates the image class labels and indices based on the folders for each audio class. We'll use the class_to_idx to view the class mapping for the image dataset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "38de17d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Class category and index of the images: {'no': 0, 'yes': 1}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class_map=yes_no_dataset.class_to_idx\n",
    "\n",
    "print(\"\\nClass category and index of the images: {}\\n\".format(class_map))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff0c2a8c",
   "metadata": {},
   "source": [
    "## Split the data for training and testing\n",
    "### We'll need to split the data to use 80 percent to train the model, and 20 percent to test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2d46489d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training size: 6388\n",
      "Testing size: 1597\n"
     ]
    }
   ],
   "source": [
    "#split data to test and train\n",
    "#use 80% to train\n",
    "train_size = int(0.8 * len(yes_no_dataset))\n",
    "test_size = len(yes_no_dataset) - train_size\n",
    "yes_no_train_dataset, yes_no_test_dataset = torch.utils.data.random_split(yes_no_dataset, [train_size, test_size])\n",
    "\n",
    "print(\"Training size:\", len(yes_no_train_dataset))\n",
    "print(\"Testing size:\",len(yes_no_test_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb58c6c",
   "metadata": {},
   "source": [
    "### Because the dataset was randomly split, let's count the training data to verify that the data has a fairly even distribution between the images in the yes and no categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "76ead897",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0: 3159, 1: 3229})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# labels in training set\n",
    "train_classes = [label for _, label in yes_no_train_dataset]\n",
    "Counter(train_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55104117",
   "metadata": {},
   "source": [
    "### Load the data into the DataLoader and specify the batch size of how the data will be divided and loaded in the training iterations. We'll also set the number of workers to specify the number of subprocesses to load the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "adf86476",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    yes_no_train_dataset,\n",
    "    batch_size=15,\n",
    "    num_workers=2,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "test_dataloader = torch.utils.data.DataLoader(\n",
    "    yes_no_test_dataset,\n",
    "    batch_size=15,\n",
    "    num_workers=2,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b063a313",
   "metadata": {},
   "source": [
    "### Let's take a look at what our training tensor looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7ff0ffc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1569, 0.2314, 0.1961, 0.1804, 0.1765, 0.1647, 0.1529, 0.1529, 0.1490,\n",
      "        0.1804, 0.1490, 0.1216, 0.2784, 0.2510, 0.2667, 0.3020, 0.3020, 0.2784,\n",
      "        0.4941, 0.2784, 0.3686, 0.1176, 0.2196, 0.2196, 0.2431, 0.2863, 0.3176,\n",
      "        0.2941, 0.2196, 0.1490, 0.2784, 0.1843, 0.1176, 0.2431, 0.3412, 0.2510,\n",
      "        0.3490, 0.4118, 0.1294, 0.2863, 0.2667, 0.1961, 0.2431, 0.1176, 0.1529,\n",
      "        0.1176, 0.1176, 0.1176, 0.1176, 0.1255, 0.1569, 0.1569, 0.1176, 0.1529,\n",
      "        0.1216, 0.1216, 0.1176, 0.1176, 0.1490, 0.1373, 0.1373, 0.1608, 0.1294,\n",
      "        0.1373, 0.1647, 0.1647, 0.1412, 0.1569, 0.1490, 0.1647, 0.1882, 0.1412,\n",
      "        0.1647, 0.1922, 0.1804, 0.1529, 0.1569, 0.1569, 0.1922, 0.1569, 0.1412])\n"
     ]
    }
   ],
   "source": [
    "td = train_dataloader.dataset[0][0][0][0]\n",
    "print(td)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "317485c0",
   "metadata": {},
   "source": [
    "### Get GPU for training, or use CPU if GPU isn't available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8c3233fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('Using {} device'.format(device))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc67088c",
   "metadata": {},
   "source": [
    "## We'll define our layers and parameters:\n",
    "\n",
    "### conv2d: Takes an input of 3 channels, which represents RGB colors because our input images are in color. The 32 represents the number of feature map images produced from the convolutional layer. The images are produced after you apply a filter on each image in a channel, with a 5 x 5 kernel size and a stride of 1. Max pooling is set with a 2 x 2 kernel size to reduce the dimensions of the filtered images. We apply the ReLU activation to replace the negative pixel values to 0.\n",
    "### conv2d: Takes the 32 output images from the previous convolutional layer as input. Then, we increase the output number to 64 feature map images, after a filter is applied on the 32 input images, with a 5 x 5 kernel size and a stride of 1. Max pooling is set with a 2 x 2 kernel size to reduce the dimensions of the filtered images. We apply the ReLU activation to replace the negative pixel values to 0.\n",
    "### dropout: Removes some of the features extracted from the conv2d layer with the ratio of 0.50, to prevent overfitting.\n",
    "### flatten: Converts features from the conv2d output image into the linear input layer.\n",
    "### Linear: Takes a number of 51136 features as input, and sets the number of outputs from the network to be 50 logits. The next layer will take the 50 inputs and produces 2 logits in the output layer. The ReLU activation function will be applied to the neurons across the linear network to replace the negative values to 0. The 2 output values will be used to predict the classification yes or no.\n",
    "### log_Softmax: An activation function applied to the 2 output values to predict the probability of the audio classification.\n",
    "### After defining the CNN, we'll set the device to run it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4d7b15fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(51136, 50)\n",
    "        self.fc2 = nn.Linear(50, 2)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        #x = x.view(x.size(0), -1)\n",
    "        x = self.flatten(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return F.log_softmax(x,dim=1)  \n",
    "\n",
    "model = CNNet().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c73712b4",
   "metadata": {},
   "source": [
    "## Create train and test functions\n",
    "### Now you set the cost function, learning rate, and optimizer. Then you define the train and test functions that you'll use to train and test the model by using the CNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b0bd7dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cost function used to determine best parameters\n",
    "cost = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# used to create optimal parameters\n",
    "learning_rate = 0.0001\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Create the training function\n",
    "\n",
    "def train(dataloader, model, loss, optimizer):\n",
    "    model.train()\n",
    "    size = len(dataloader.dataset)\n",
    "    for batch, (X, Y) in enumerate(dataloader):\n",
    "        \n",
    "        X, Y = X.to(device), Y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(X)\n",
    "        loss = cost(pred, Y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f'loss: {loss:>7f}  [{current:>5d}/{size:>5d}]')\n",
    "\n",
    "\n",
    "# Create the validation/test function\n",
    "\n",
    "def test(dataloader, model):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch, (X, Y) in enumerate(dataloader):\n",
    "            X, Y = X.to(device), Y.to(device)\n",
    "            pred = model(X)\n",
    "\n",
    "            test_loss += cost(pred, Y).item()\n",
    "            correct += (pred.argmax(1)==Y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= size\n",
    "    correct /= size\n",
    "\n",
    "    print(f'\\nTest Error:\\nacc: {(100*correct):>0.1f}%, avg loss: {test_loss:>8f}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a2c716a",
   "metadata": {},
   "source": [
    "## Train the model\n",
    "### Now let's set the number of epochs, and call our train and test functions for each iteration. We'll iterate through the training network by the number of epochs. As we train the model, we'll calculate the loss as it decreases during the training. In addition, we'll display the accuracy as the optimization increases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "78fc2ff5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 0.693605  [    0/ 6388]\n",
      "loss: 0.763734  [ 1500/ 6388]\n",
      "loss: 0.240859  [ 3000/ 6388]\n",
      "loss: 0.420651  [ 4500/ 6388]\n",
      "loss: 0.431145  [ 6000/ 6388]\n",
      "\n",
      "Test Error:\n",
      "acc: 90.5%, avg loss: 0.014709\n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.189520  [    0/ 6388]\n",
      "loss: 0.210554  [ 1500/ 6388]\n",
      "loss: 0.071000  [ 3000/ 6388]\n",
      "loss: 0.178050  [ 4500/ 6388]\n",
      "loss: 0.061309  [ 6000/ 6388]\n",
      "\n",
      "Test Error:\n",
      "acc: 92.8%, avg loss: 0.012278\n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.115656  [    0/ 6388]\n",
      "loss: 0.192352  [ 1500/ 6388]\n",
      "loss: 0.388241  [ 3000/ 6388]\n",
      "loss: 0.065765  [ 4500/ 6388]\n",
      "loss: 0.223373  [ 6000/ 6388]\n",
      "\n",
      "Test Error:\n",
      "acc: 93.4%, avg loss: 0.010371\n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.251407  [    0/ 6388]\n",
      "loss: 0.406289  [ 1500/ 6388]\n",
      "loss: 0.211145  [ 3000/ 6388]\n",
      "loss: 0.099378  [ 4500/ 6388]\n",
      "loss: 0.072085  [ 6000/ 6388]\n",
      "\n",
      "Test Error:\n",
      "acc: 93.2%, avg loss: 0.010182\n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.258044  [    0/ 6388]\n",
      "loss: 0.201129  [ 1500/ 6388]\n",
      "loss: 0.247161  [ 3000/ 6388]\n",
      "loss: 0.132029  [ 4500/ 6388]\n",
      "loss: 0.107002  [ 6000/ 6388]\n",
      "\n",
      "Test Error:\n",
      "acc: 94.2%, avg loss: 0.008924\n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 0.103633  [    0/ 6388]\n",
      "loss: 0.121764  [ 1500/ 6388]\n",
      "loss: 0.307170  [ 3000/ 6388]\n",
      "loss: 0.300484  [ 4500/ 6388]\n",
      "loss: 0.130474  [ 6000/ 6388]\n",
      "\n",
      "Test Error:\n",
      "acc: 94.5%, avg loss: 0.008277\n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 0.126685  [    0/ 6388]\n",
      "loss: 0.149534  [ 1500/ 6388]\n",
      "loss: 0.038820  [ 3000/ 6388]\n",
      "loss: 0.395370  [ 4500/ 6388]\n",
      "loss: 0.024135  [ 6000/ 6388]\n",
      "\n",
      "Test Error:\n",
      "acc: 95.0%, avg loss: 0.007984\n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.069547  [    0/ 6388]\n",
      "loss: 0.053527  [ 1500/ 6388]\n",
      "loss: 0.171041  [ 3000/ 6388]\n",
      "loss: 0.145324  [ 4500/ 6388]\n",
      "loss: 0.087393  [ 6000/ 6388]\n",
      "\n",
      "Test Error:\n",
      "acc: 95.2%, avg loss: 0.007088\n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.092401  [    0/ 6388]\n",
      "loss: 0.051666  [ 1500/ 6388]\n",
      "loss: 0.082023  [ 3000/ 6388]\n",
      "loss: 0.131317  [ 4500/ 6388]\n",
      "loss: 0.072916  [ 6000/ 6388]\n",
      "\n",
      "Test Error:\n",
      "acc: 95.1%, avg loss: 0.007137\n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.072144  [    0/ 6388]\n",
      "loss: 0.350897  [ 1500/ 6388]\n",
      "loss: 0.037031  [ 3000/ 6388]\n",
      "loss: 0.040512  [ 4500/ 6388]\n",
      "loss: 0.203647  [ 6000/ 6388]\n",
      "\n",
      "Test Error:\n",
      "acc: 95.0%, avg loss: 0.007623\n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 0.033235  [    0/ 6388]\n",
      "loss: 0.148367  [ 1500/ 6388]\n",
      "loss: 0.081365  [ 3000/ 6388]\n",
      "loss: 0.063966  [ 4500/ 6388]\n",
      "loss: 0.101605  [ 6000/ 6388]\n",
      "\n",
      "Test Error:\n",
      "acc: 94.7%, avg loss: 0.007393\n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 0.014442  [    0/ 6388]\n",
      "loss: 0.090830  [ 1500/ 6388]\n",
      "loss: 0.320638  [ 3000/ 6388]\n",
      "loss: 0.339466  [ 4500/ 6388]\n",
      "loss: 0.286531  [ 6000/ 6388]\n",
      "\n",
      "Test Error:\n",
      "acc: 95.1%, avg loss: 0.006932\n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 0.099906  [    0/ 6388]\n",
      "loss: 0.032637  [ 1500/ 6388]\n",
      "loss: 0.035330  [ 3000/ 6388]\n",
      "loss: 0.167739  [ 4500/ 6388]\n",
      "loss: 0.051535  [ 6000/ 6388]\n",
      "\n",
      "Test Error:\n",
      "acc: 95.6%, avg loss: 0.006732\n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 0.050161  [    0/ 6388]\n",
      "loss: 0.528611  [ 1500/ 6388]\n",
      "loss: 0.120010  [ 3000/ 6388]\n",
      "loss: 0.123939  [ 4500/ 6388]\n",
      "loss: 0.251181  [ 6000/ 6388]\n",
      "\n",
      "Test Error:\n",
      "acc: 95.6%, avg loss: 0.006912\n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 0.227793  [    0/ 6388]\n",
      "loss: 0.025420  [ 1500/ 6388]\n",
      "loss: 0.152094  [ 3000/ 6388]\n",
      "loss: 0.100538  [ 4500/ 6388]\n",
      "loss: 0.072758  [ 6000/ 6388]\n",
      "\n",
      "Test Error:\n",
      "acc: 95.5%, avg loss: 0.006639\n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "epochs = 15\n",
    "\n",
    "for t in range(epochs):\n",
    "    print(f'Epoch {t+1}\\n-------------------------------')\n",
    "    train(train_dataloader, model, cost, optimizer)\n",
    "    test(test_dataloader, model)\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe80de8f",
   "metadata": {},
   "source": [
    "### Let's look at the summary breakdown of the model architecture. It shows the number of filters used for the feature extraction and image reduction from pooling for each convolutional layer. Next, it shows 51136 input features and the 2 outputs used for classification in the linear layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ffd9bf25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "CNNet                                    [15, 2]                   --\n",
       "├─Conv2d: 1-1                            [15, 32, 197, 77]         2,432\n",
       "├─Conv2d: 1-2                            [15, 64, 94, 34]          51,264\n",
       "├─Dropout2d: 1-3                         [15, 64, 94, 34]          --\n",
       "├─Flatten: 1-4                           [15, 51136]               --\n",
       "├─Linear: 1-5                            [15, 50]                  2,556,850\n",
       "├─Linear: 1-6                            [15, 2]                   102\n",
       "==========================================================================================\n",
       "Total params: 2,610,648\n",
       "Trainable params: 2,610,648\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 3.05\n",
       "==========================================================================================\n",
       "Input size (MB): 2.93\n",
       "Forward/backward pass size (MB): 82.80\n",
       "Params size (MB): 10.44\n",
       "Estimated Total Size (MB): 96.17\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(model, input_size=(15, 3, 201, 81))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd808d73",
   "metadata": {},
   "source": [
    "## Test the model\n",
    "### You should have got somewhere between a 93-95 percent accuracy by the 15th epoch. Here we grab a batch from our test data, and see how the model performs on the predicted result and the actual result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2ab153de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted:\n",
      "value=0, class_name= no\n",
      "\n",
      "Actual:\n",
      "value=0, class_name= no\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "test_loss, correct = 0, 0\n",
    "class_map = ['no', 'yes']\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch, (X, Y) in enumerate(test_dataloader):\n",
    "        X, Y = X.to(device), Y.to(device)\n",
    "        pred = model(X)\n",
    "        print(\"Predicted:\\nvalue={}, class_name= {}\\n\".format(pred[0].argmax(0),class_map[pred[0].argmax(0)]))\n",
    "        print(\"Actual:\\nvalue={}, class_name= {}\\n\".format(Y[0],class_map[Y[0]]))\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea49e260",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f99639c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
